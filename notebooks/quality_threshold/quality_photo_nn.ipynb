{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import PIL\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "RADNDOM_STATE = 42\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формирование датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>quality_photo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\..\\data\\sorted_data_merged\\damaged\\1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\..\\data\\sorted_data_merged\\damaged\\2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\..\\data\\sorted_data_merged\\damaged\\3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\..\\data\\sorted_data_merged\\damaged\\4.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\..\\data\\sorted_data_merged\\damaged\\5.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     file_name  quality_photo\n",
       "0  ..\\..\\data\\sorted_data_merged\\damaged\\1.jpg              1\n",
       "1  ..\\..\\data\\sorted_data_merged\\damaged\\2.jpg              1\n",
       "2  ..\\..\\data\\sorted_data_merged\\damaged\\3.jpg              1\n",
       "3  ..\\..\\data\\sorted_data_merged\\damaged\\4.jpg              1\n",
       "4  ..\\..\\data\\sorted_data_merged\\damaged\\5.jpg              1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('../../data/common_files/df_markup.pkl')\n",
    "df['file_name'] = df['file_name'].apply(\n",
    "    lambda x: \"..\\\\\" + x\n",
    ")\n",
    "df = df[['file_name', 'quality_photo']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, stratify=df['quality_photo'], random_state=RADNDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "test_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "        if not os.path.exists('data/train/quality_positive'):\n",
    "            os.makedirs('data/train/quality_positive')\n",
    "        if not os.path.exists('data/train/quality_negative'):\n",
    "            os.makedirs('data/train/quality_negative')\n",
    "            \n",
    "    if not os.path.exists(test_dir):\n",
    "        os.makedirs(test_dir)\n",
    "        if not os.path.exists('data/test/quality_positive'):\n",
    "            os.makedirs('data/test/quality_positive')\n",
    "        if not os.path.exists('data/test/quality_negative'):\n",
    "            os.makedirs('data/test/quality_negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5969/5969 [00:04<00:00, 1411.18it/s]\n",
      "100%|██████████| 415/415 [00:00<00:00, 1583.45it/s]\n",
      "100%|██████████| 1989/1989 [00:01<00:00, 1601.29it/s]\n",
      "100%|██████████| 139/139 [00:00<00:00, 1775.52it/s]\n"
     ]
    }
   ],
   "source": [
    "train_positive = train[train['quality_photo']==1]\n",
    "train_negative = train[train['quality_photo']==0]\n",
    "\n",
    "for file in tqdm(train_positive['file_name']):\n",
    "    shutil.copy(file, 'data/train/quality_positive')\n",
    "for file in tqdm(train_negative['file_name']):\n",
    "    shutil.copy(file, 'data/train/quality_negative')\n",
    "\n",
    "test_positive = test[test['quality_photo']==1]\n",
    "test_negative = test[test['quality_photo']==0]\n",
    "\n",
    "for file in tqdm(test_positive['file_name']):\n",
    "    shutil.copy(file, 'data/test/quality_positive')\n",
    "for file in tqdm(test_negative['file_name']):\n",
    "    shutil.copy(file, 'data/test/quality_negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "test_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 6383\n",
      "    Root location: data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomRotation(degrees=[-30.0, 30.0], interpolation=nearest, expand=False, fill=0)\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear), antialias=None)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "Test data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 2128\n",
      "    Root location: data/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n",
    "                                  transform=train_transforms, # transforms to perform on data (images)\n",
    "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
    "test_data = datasets.ImageFolder(root=test_dir, \n",
    "                                 transform=test_transforms)\n",
    "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quality_negative': 0, 'quality_positive': 1}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also get class names as a dict\n",
    "class_dict = train_data.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoader's with batch size 32 and 20 workers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x22668a302e0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x22668a30070>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup batch size and number of workers \n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n",
    "\n",
    "# Create DataLoader's\n",
    "train_dataloader = DataLoader(train_data, \n",
    "                                     batch_size=BATCH_SIZE, \n",
    "                                     shuffle=True, \n",
    "                                     num_workers=NUM_WORKERS)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, \n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    shuffle=False, \n",
    "                                    num_workers=NUM_WORKERS)\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, test_losses, train_accuracies, test_accuracies, train_f1s, test_f1s):\n",
    "    clear_output()\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 10))\n",
    "    plt.rcParams['font.size'] = '12'\n",
    "    axs[0].plot(range(1, len(train_losses) + 1), train_losses, label='train')\n",
    "    axs[0].plot(range(1, len(test_losses) + 1), test_losses, label='test')\n",
    "    axs[0].set_ylabel('loss')\n",
    "\n",
    "    axs[1].plot(range(1, len(train_accuracies) + 1), train_accuracies, label='train')\n",
    "    axs[1].plot(range(1, len(test_accuracies) + 1), test_accuracies, label='test')\n",
    "    axs[1].set_ylabel('accuracy')\n",
    "\n",
    "    axs[2].plot(range(1, len(train_f1s) + 1), train_f1s, label='train')\n",
    "    axs[2].plot(range(1, len(test_f1s) + 1), test_f1s, label='test')\n",
    "    axs[2].set_ylabel('F1')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(model, optimizer, criterion, train_loader, tqdm_desc):\n",
    "    train_loss, train_accuracy, f1_score_value = 0.0, 0.0, 0.0\n",
    "    model.train()\n",
    "    for images, labels in tqdm(train_loader, desc=tqdm_desc):\n",
    "        images = images.to(device)  # images: batch_size x num_channels x height x width\n",
    "        labels = labels.to(device)  # labels: batch_size\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)  # logits: batch_size x num_classes\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.shape[0]\n",
    "        train_accuracy += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        # Calculate F1-score\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        f1_score_value += f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy /= len(train_loader.dataset)\n",
    "    f1_score_value /= len(train_loader)\n",
    "    \n",
    "    return train_loss, train_accuracy, f1_score_value\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_epoch(model, criterion, test_loader, tqdm_desc):\n",
    "    test_loss, test_accuracy, test_f1 = 0.0, 0.0, 0.0\n",
    "    model.eval()\n",
    "    for images, labels in tqdm(test_loader, desc=tqdm_desc):\n",
    "        images = images.to(device)  # images: batch_size x num_channels x height x width\n",
    "        labels = labels.to(device)  # labels: batch_size\n",
    "        logits = model(images)  # logits: batch_size x num_classes\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        test_loss += loss.item() * images.shape[0]\n",
    "        test_accuracy += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        # Calculate F1-score\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        test_f1 += f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy /= len(test_loader.dataset)\n",
    "    test_f1 /= len(test_loader)\n",
    "\n",
    "    return test_loss, test_accuracy, test_f1\n",
    "\n",
    "    \n",
    "def train(model, optimizer, scheduler, criterion, train_loader, test_loader, num_epochs):\n",
    "    train_losses, train_accuracies, train_f1s = [], [], []\n",
    "    test_losses, test_accuracies, test_f1s = [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_accuracy, train_f1 = training_epoch(\n",
    "            model, optimizer, criterion, train_loader,\n",
    "            tqdm_desc=f'Training {epoch}/{num_epochs}'\n",
    "        )\n",
    "        test_loss, test_accuracy, test_f1 = validation_epoch(\n",
    "            model, criterion, test_loader,\n",
    "            tqdm_desc=f'Validating {epoch}/{num_epochs}'\n",
    "        )\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_losses += [train_loss]\n",
    "        train_accuracies += [train_accuracy]\n",
    "        train_f1s += [train_f1]\n",
    "        test_losses += [test_loss]\n",
    "        test_accuracies += [test_accuracy]\n",
    "        test_f1s += [test_f1]\n",
    "        plot_losses(train_losses, test_losses, train_accuracies, test_accuracies, train_f1s, test_f1s)\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies, train_f1s, test_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet152()\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                        ('fc1', nn.Linear(2048, 1024)),\n",
    "                        ('relu', nn.ReLU()),\n",
    "                        ('fc2', nn.Linear(1024, 2)),\n",
    "                        ('output', nn.LogSoftmax(dim=1))\n",
    "                        ]))\n",
    "\n",
    "# Replacing the pretrained model classifier with our classifier\n",
    "model.fc = classifier\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1/10:  36%|███▋      | 73/200 [23:38<41:07, 19.43s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\projects\\ai_acts\\notebooks\\quality_threshold\\quality_photo_nn.ipynb Cell 19\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_losses, test_losses, train_accuracies, test_accuracies, train_f1s, test_f1s \u001b[39m=\u001b[39m train(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model, optimizer, scheduler, criterion, train_dataloader, test_dataloader, num_epochs\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "\u001b[1;32md:\\projects\\ai_acts\\notebooks\\quality_threshold\\quality_photo_nn.ipynb Cell 19\u001b[0m line \u001b[0;36mtrain\u001b[1;34m(model, optimizer, scheduler, criterion, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m test_losses, test_accuracies, test_f1s \u001b[39m=\u001b[39m [], [], []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     train_loss, train_accuracy, train_f1 \u001b[39m=\u001b[39m training_epoch(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m         model, optimizer, criterion, train_loader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m         tqdm_desc\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTraining \u001b[39;49m\u001b[39m{\u001b[39;49;00mepoch\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mnum_epochs\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     test_loss, test_accuracy, test_f1 \u001b[39m=\u001b[39m validation_epoch(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m         model, criterion, test_loader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m         tqdm_desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidating \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39mif\u001b[39;00m scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32md:\\projects\\ai_acts\\notebooks\\quality_threshold\\quality_photo_nn.ipynb Cell 19\u001b[0m line \u001b[0;36mtraining_epoch\u001b[1;34m(model, optimizer, criterion, train_loader, tqdm_desc)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m logits \u001b[39m=\u001b[39m model(images)  \u001b[39m# logits: batch_size x num_classes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/projects/ai_acts/notebooks/quality_threshold/quality_photo_nn.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m images\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "train_losses, test_losses, train_accuracies, test_accuracies, train_f1s, test_f1s = train(\n",
    "    model, optimizer, scheduler, criterion, train_dataloader, test_dataloader, num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не сработало :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
